{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Output \"dense_5\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_5\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          200960      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            514         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            514         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          768         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 784)          201488      dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 404,244\n",
      "Trainable params: 404,244\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 9s 1us/step\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 191.2641 - val_loss: 173.2601\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 171.1513 - val_loss: 168.3204\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 167.4142 - val_loss: 165.7579\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 164.7169 - val_loss: 163.2605\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 162.9733 - val_loss: 162.1767\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 161.7978 - val_loss: 161.5374\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 160.9462 - val_loss: 160.4070\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 160.2219 - val_loss: 160.2311\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 159.6169 - val_loss: 159.8009\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 159.1062 - val_loss: 159.5684\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 158.6063 - val_loss: 158.7118\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 158.1360 - val_loss: 158.6823\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 157.7402 - val_loss: 158.1436\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 157.3421 - val_loss: 158.1158\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 156.9929 - val_loss: 157.3773\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 17s 277us/step - loss: 156.6449 - val_loss: 157.4319\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 156.3421 - val_loss: 157.0481\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 156.0379 - val_loss: 157.0062\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 17s 282us/step - loss: 155.7550 - val_loss: 156.7008\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 155.4607 - val_loss: 156.6100\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 155.1916 - val_loss: 156.3589\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 154.9460 - val_loss: 155.8892\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 154.7184 - val_loss: 155.8370\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 154.4625 - val_loss: 155.8442\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 154.2074 - val_loss: 155.2599\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 153.9696 - val_loss: 155.5258\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 153.7712 - val_loss: 155.2788\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 153.5801 - val_loss: 154.9443\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 153.3642 - val_loss: 154.9662\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 153.2036 - val_loss: 155.5460\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 19s 313us/step - loss: 152.9885 - val_loss: 155.0318\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 152.8282 - val_loss: 154.4989\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 152.6597 - val_loss: 155.0073\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 152.4853 - val_loss: 154.2077\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 152.3221 - val_loss: 154.0345\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 152.1457 - val_loss: 154.0170\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 152.0370 - val_loss: 154.0228\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 151.8850 - val_loss: 153.5992\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 151.7563 - val_loss: 154.3900\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 151.5836 - val_loss: 153.5916\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 151.4952 - val_loss: 153.4035\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 151.3497 - val_loss: 153.3286\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 151.2291 - val_loss: 153.5024\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 151.1005 - val_loss: 153.3343\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 150.9999 - val_loss: 153.9405\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 150.8823 - val_loss: 153.3485\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 150.7656 - val_loss: 153.2432\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 150.6535 - val_loss: 153.2695\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 150.5534 - val_loss: 153.3557\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 150.4527 - val_loss: 153.0362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c735fb7e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c735ef6f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 100\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# instantiate VAE model\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# Compute VAE loss\n",
    "xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "# train the VAE on MNIST digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "vae.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
